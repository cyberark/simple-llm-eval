{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Simple-Eval","text":"<p>Welcome to the simple LLM evaluation framework.</p> <p>You can use simpleval to evaluate your LLMs with a simple and easy-to-use CLI.</p> <p> \ud83d\ude80 Head over to Quickstart to get started. </p>"},{"location":"#whats-in-the-box","title":"What's In The Box? \ud83c\udf81","text":"<p>The built-in evaluation uses the \"LLM as a Judge\" technique, implemented with various LLM providers\u2014learn more about it here.</p>"},{"location":"#why-simple-eval","title":"Why Simple-Eval?","text":"<p>It is dead simple and easily customizable.</p> <p>Often using an evaluation platform is an overkill, and other frameworks did not match the simple requirements we had. And so, simpleval was born.</p>"},{"location":"#further-reading","title":"Further Reading \ud83d\udcda","text":"<p>If you want to deep dive into the \"LLM As a Judge\" evaluation technique, some relevant papers are:</p> <ul> <li> <p>LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</p> </li> <li> <p>A Survey on LLM-as-a-Judge</p> </li> </ul> <p>Happy reading!</p> <p></p>"},{"location":"developers/debugging/","title":"Debugging Simpleval \ud83d\udc1e","text":""},{"location":"developers/debugging/#vscode-debug-settings","title":"vscode Debug settings","text":"<p>Example for configuring vscode to debug a run command overwriting existing results:</p> <p>Filename: <code>launch.json</code> <pre><code>{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Debug simpleval\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/simpleval/main.py\",\n      \"console\": \"integratedTerminal\",\n      \"args\": [\n        \"run\",\n        \"-e\",\n        \"simpleval/eval_sets/user-actions-by-events\",\n        \"-t\",\n        \"sonnet35\",\n        \"-o\",\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"developers/debugging/#jetbrains-debug-settings","title":"JetBrains Debug settings","text":"<p>Download and install vscode: https://code.visualstudio.com/ Just kidding \ud83d\ude1c, you got this! just run debug for the <code>simpleval/main.py</code> with the needed args.</p> <p></p>"},{"location":"developers/dev-notes/","title":"Dev Notes \ud83d\udc69\u200d\ud83d\udcbb","text":""},{"location":"developers/dev-notes/#breaking-changes","title":"Breaking Changes \u26d3\ufe0f\u200d\ud83d\udca5","text":"<p>Danger</p> <p>Make sure you don't introduce breaking changes unless you have to (in which case up the major version)</p> <p>Breaking changes affect the implementation of task_handler.py</p> <p>Specifically:</p> <ul> <li>The name of the file must be <code>task_handler.py</code></li> <li>The signature of the <code>task_logic</code> function must be <code>task_logic(name: str, payload: dict) -&gt; LlmTaskResult</code></li> <li>Handlers are expected to use the <code>simpleval.utilities.retryables.bedrock_limits_retry</code> decorator as is.</li> <li>The structure of the <code>ground_truth.jsonl</code> file must not change (see <code>base_eval_case_schema.GroundTruth</code>).</li> <li>The structure of result files must not change (see <code>eval_result_schema.EvalTestResult</code>, <code>llm_task_result.LlmTaskResult</code>).</li> </ul>"},{"location":"developers/dev-notes/#pre-requisites","title":"Pre-requisites \ud83d\udee0\ufe0f","text":"<ul> <li>Python 3.11 or higher</li> <li>The <code>uv</code> package manager</li> <li>Only for reporting dev: <code>npm</code></li> </ul>"},{"location":"developers/dev-notes/#getting-started","title":"Getting Started \ud83d\ude80","text":"<ul> <li>Clone this repo</li> <li>Install all dependencies: <code>uv sync</code></li> <li>Activate the virtual environment: <code>source .venv/bin/activate</code> (Linux/Mac) or <code>.venv\\Scripts\\activate</code> (Windows)</li> </ul>"},{"location":"developers/dev-notes/#running-all-checks","title":"Running all checks \ud83d\udd0d","text":"<p>Main checks</p> <p>Run the the <code>ci/run_checks.py</code> script This will run the following checks:</p> <ul> <li>pre-commit hooks: formatting and ruff</li> <li>ruff covers linting, formatting, isort and seurity checks</li> <li>radon and xenon for code complexity</li> <li>Code coverage checks - must be above 90%</li> <li>Verify uv.lock and requirements.txt are synced</li> </ul> <p>Optionally also run <code>./ci/run_code_cov.py</code> to make sure the code coverage is adequate.</p> <p>React checks</p> <p>If you made changes to the <code>reports-frontend</code> reports, run the <code>ci/run_checks_react.py</code> script   This will run the following steps:</p> <ul> <li>npm install, build</li> <li>run the tests</li> <li>npm audit</li> </ul> <p>The same checks will run in the pipeline as well.</p>"},{"location":"developers/dev-notes/#update-the-pre-commit-hooks","title":"Update the pre-commit hooks","text":"<p>Once in a while update the pre-commit hooks to the latest version:</p> <p><code>pre-commit autoupdate --repo https://github.com/pre-commit/pre-commit-hooks --config .config/.pre-commit-config.yaml</code></p>"},{"location":"developers/dev-notes/#building-the-package","title":"Building the Package \ud83c\udfd7\ufe0f","text":"<ul> <li>Build the package using: <code>uv build</code></li> </ul>"},{"location":"developers/dev-notes/#useful-uv-commands","title":"Useful uv Commands \ud83d\udee0\ufe0f","text":"<ul> <li>Install all dependencies: <code>uv sync</code></li> <li>Update one package: <code>uv lock --upgrade-package &lt;package-name&gt; &amp;&amp; uv sync</code></li> <li>Update all packages: <code>uv lock --upgrade &amp;&amp; uv sync</code></li> <li>Update dependencies constraints in <code>pyproject.toml</code> as needed</li> </ul>"},{"location":"developers/dev-notes/#update-npm-packages","title":"Update npm packages \ud83d\udce6","text":"<ul> <li><code>cd reports-frontend</code></li> <li><code>npm update</code></li> </ul>"},{"location":"developers/dev-notes/#publishing-the-docs","title":"Publishing the docs \ud83d\udcda","text":"<p>This project uses mkdocs/mike to publish its docs to GitHub Pages on a release.</p> <p>What is Published Automatically</p> <p>The release workflow will publish the docs but only for release versions, not for pre-release versions.</p> <p>To publish docs for pre-release versions, you can trigger the <code>Docs Release Manual</code> workflow manually.</p> <p>Here are some details on how to use mike and mkdocs:</p> <ul> <li>To test the docs locally, run: <code>mike serve</code> or <code>mkdocs serve</code></li> <li>The pipeline publishes the docs after a successful merge to main.</li> <li>There is also a manual docs publish workflow: <code>.github/workflows/docs-release.yml</code></li> <li>Version support is handled by <code>mike</code>, see the <code>mkdocs.yml</code> file, see: https://github.com/jimporter/mike/blob/master/README.md</li> </ul>"},{"location":"developers/dev-notes/#deploying-the-docs-performed-by-the-pipeline","title":"Deploying the docs (performed by the pipeline)","text":"<ul> <li>Deploy and push docs changes: <code>mike deploy [version] [alias] --push</code>, for example: <code>mike deploy 0.1 latest --push</code></li> <li>Deploy and push alias update: <code>mike deploy --push --update-aliases &lt;version&gt; latest</code></li> <li>IMPORTANT: <code>mike set-default --push latest</code>, to set the default version to latest</li> </ul>"},{"location":"developers/dev-notes/#deleting-docs-versions","title":"Deleting docs versions","text":"<ul> <li>To delete a version: <code>mike delete 0.1 --push --message \"Delete version 0.1\" [--update-aliases latest]</code></li> <li>Delete all docs versions: <code>mike delete --all --push</code></li> </ul>"},{"location":"developers/dev-notes/#cli-dev","title":"CLI Dev","text":"<p>The CLI uses the click library, see commands package for examples.</p> <p>NOTE: When updating the CLI commands, make sure to run <code>uv install</code> to install the dev package so you can run the CLI locally with the latest changes.</p>"},{"location":"developers/dev-notes/#llm-as-a-judge","title":"LLM as a Judge \ud83d\udc69\u200d\u2696\ufe0f","text":"<p><code>simpleval</code> comes with a bunch of judges out of the box: one uses boto3 natively for Bedrock, and all others use Lite LLM to unify the interface for any LLM provider.</p> <p>It is recommended to use the Lite LLM interface for any new judge you want to implement, as it is more flexible and allows you to use any LLM provider. See <code>simpleval/evaluation/judges/models/open_ai/judge.py</code> for an example of how to implement a judge using Lite LLM, which is super easy. All Lite LLM-based judges use the same prompts for metrics.</p> <p>Note</p> <ul> <li>Lite LLM judges must use models that support structured outputs. Use the <code>simpleval litellm-models-explorer</code> to find such models by provider.</li> <li>If you implement a new judge type, make sure you implement a retry decorator (see existing judges for examples).</li> </ul>"},{"location":"developers/reporting-dev/","title":"Reporting Dev \ud83d\udcca\ud83e\uddd1\u200d\ud83d\udcbb","text":""},{"location":"developers/reporting-dev/#prerequisites","title":"Prerequisites \ud83d\udee0","text":"<ul> <li><code>Node.js</code> (v18.20.6 or higher)</li> <li><code>npm</code> \ufe0f</li> </ul>"},{"location":"developers/reporting-dev/#building-the-reports","title":"Building the Reports \ud83c\udfd7","text":"<ul> <li>Run <code>cd reports-frontend</code></li> <li>Install packages: <code>npm install</code></li> <li>Build all reports: <code>npm run build</code></li> <li>Build single reports:<ul> <li><code>npm run build:eval</code></li> <li><code>npm run build:compare</code></li> <li><code>npm run build:summary</code></li> </ul> </li> </ul>"},{"location":"developers/reporting-dev/#running-the-tests","title":"Running the Tests \ud83e\uddea","text":"<ul> <li>Run tests (watch): <code>npm run test</code></li> <li>Run tests (once): <code>npm run test-no-watch</code></li> </ul>"},{"location":"developers/reporting-dev/#new-reports","title":"New Reports \ud83e\udd16\ud83d\udcca","text":"<p>The new reports are implemented with React with vite as the build tool. Each report is a standalone React app that is built into a single, self contained HTML file. This means that you can simply load the html file without needing to run a server. It does not use TS, but it probably should eventually.</p> <p>The react project is in <code>simpleval/reports-frontend</code></p> <p>Each such react app generates an HTML template that needs to be populated with data by simpleval and then saved to disk.</p> <p>The results and compare reports use native react, and the summary report use Material UI.</p> <p>There are some internal notes here: <code>reports-frontend/README.md</code></p>"},{"location":"developers/reporting-dev/#populating-the-templates","title":"Populating the Templates \ud83d\udcdd\ud83d\udcca","text":"<p>Take a look at <code>reports-frontend/src/components/LLMEvalReport.jsx</code> You will see mock data that is used during testing. In this case they are:</p> <ul> <li><code>rowsData</code></li> <li><code>aggregateData</code></li> <li><code>errors</code></li> </ul> <p>Those needs to be overwritten by simpleval with the actual data. To do that, we have injection scripts thar run during build and create the html templates with data placeholders. Each component has a corresponding injection script in <code>reports-frontend/scripts/</code></p> <p>In our example look at: <code>reports-frontend/scripts/inject_placeholders_eval.cjs</code></p> <p>It defines the data placeholders to inject to the html (and replace the mock data) and also where to copy the template to (This is the place that simpleval expects to find it - in our example - <code>simpleval/commands/reporting/eval_report/html2</code>).</p> <p>In simpleval we need to know these placeholder and replace them with the actual data. See: <code>simpleval/commands/reporting/eval_report/html2/html2_report.py</code></p>"},{"location":"developers/reporting-dev/#adding-a-new-report","title":"Adding a New Report \ud83d\ude80\ud83d\udcca","text":"<p>Going through all the points here let you know about the important elements of the new reports.</p>"},{"location":"developers/reporting-dev/#create-a-new-component-file","title":"Create a new component file","text":"<ul> <li>Create a new component file in <code>reports-frontend/src/components</code>, you can copy one of the existing files, for this example let's say we copy <code>SummaryReport.jsx</code></li> <li>Rename the file, in it rename all the relevant parts (e.g. <code>SummaryReportControl</code>, <code>SummaryReport</code> - this can change depending on the report you copied)</li> <li>Remove all unwanted code (generally keep only the icon and the title)</li> </ul>"},{"location":"developers/reporting-dev/#create-a-main-file","title":"Create a Main File","text":"<ul> <li>copy <code>reports-frontend/src/summaryMain.jsx</code> and rename it.</li> <li>update the import to your new component (don't forget to update both the name of the component and the import)</li> </ul>"},{"location":"developers/reporting-dev/#create-an-html-file","title":"Create an html File","text":"<ul> <li>copy <code>summaryReport.html</code> and rename it.</li> <li>update the import to your new main file.</li> <li>update the title</li> </ul>"},{"location":"developers/reporting-dev/#create-an-injection-file","title":"Create an Injection File","text":"<ul> <li>create an injection script in <code>reports-frontend/scripts/</code></li> <li>copy one of the existing scripts and rename it.</li> <li>update the placeholders according to your implementation.</li> <li>update the destination folder (This is where simpleval code expects to find the template)</li> </ul>"},{"location":"developers/reporting-dev/#update-packagejson","title":"Update <code>package.json</code>","text":"<ul> <li>create a build step: e.g. copy <code>\"build:summary\"</code> and rename it.</li> <li>update <code>build</code> to include your new build step.</li> <li>create a new postbuild step: copy for example <code>postbuild:summary</code> and update as needed.</li> </ul>"},{"location":"developers/reporting-dev/#component-level-vite-config-file","title":"Component Level <code>vite</code> Config File","text":"<ul> <li>copy <code>vite.config.summary.js</code> and rename it for you component</li> <li>update <code>build.rollupOptions.input</code> to match your new component HTML file.  </li> <li>update <code>build.outDir</code> to match your new component output directory.</li> </ul>"},{"location":"developers/reporting-dev/#global-vite-config","title":"Global <code>vite</code> Config","text":"<ul> <li>update <code>vite.config.js</code> - add your new vite config file to <code>build.rollupOptions.input</code></li> </ul>"},{"location":"developers/reporting-dev/#component-implementation","title":"Component Implementation","text":"<ul> <li>Implement your component</li> <li>If you want an example of a react native component, look at <code>LLMEvalReport.jsx</code> or <code>CompareReport.jsx</code></li> <li>If you want an example of a Material UI component, look at <code>SummaryReport.jsx</code></li> <li>Implement tests - see <code>LLMEvalReport.test.jsx</code> for an example of a react native component test, as a minimum , test that the component renders correctly.</li> </ul>"},{"location":"developers/testing-judges/","title":"Testing the judges \ud83d\udc69\u200d\u2696\ufe0f\ud83e\uddd1\u200d\u2696\ufe0f","text":""},{"location":"developers/testing-judges/#minimal-judge-tests","title":"Minimal Judge Tests","text":"<p>A minimal test to verify that the main judges are working as expected is include here:</p> <p><code>tests/integration/test_judges_minimal.py:test_judges_minimal</code></p> <p>Since it consumes LLM tokens, it does not run by default. It requires credentials for all relevant providers.</p> <p>To run these tests set this environment variable:</p> <p><code>RUN_ALL_MINIMAL_JUDGE_TESTS=1</code></p> <p>For example:</p> <pre><code>RUN_ALL_MINIMAL_JUDGE_TESTS=1 pytest -v tests/integration/test_judges_minimal.py\n</code></pre> <p>There are also provider specific environment variables (see in code).</p>"},{"location":"developers/testing-judges/#in-depth-judge-tests","title":"In-Depth Judge Tests","text":"<p>To make sure that the LLM as a judge implementation is working as expected, you can run the judges on a set of evaluation sets for three use cases: \"Classify products\", \"Detect Toxicity\" and \"Spam Detection\".</p> <p>Info</p> <p>The ground truth for each evaluation is kept here, by provider: <code>tests/resources/llm_as_a_judge_test_eval_results_golden_set</code></p> <p>The last results for each evaluation and provider and the config file needed to run the eval are kept here: <code>tests/resources/llm_as_a_judge_datasets</code></p> <p>The <code>tests/integration/test_llm_as_a_judge.py</code> integration test runs in the pipeline, but it compares the static results with the ground truth. In case there are changes in the prompt, models used, model parameters, etc., you should run the evaluation again for the relevant judge(s) and update the static results.</p>"},{"location":"developers/testing-judges/#running-the-evaluations","title":"Running the Evaluations \ud83c\udfc3\u200d\u2642\ufe0f\u200d\u27a1\ufe0f","text":"<p>You can run all evaluation for a judge with this script: <pre><code>python tools/run_all_llm_as_a_judge_tests.py --config-file &lt;config&gt; \n</code></pre></p> <p>The code retries on errors, but if you still get temporary errors (like throttling, parsing issues), simply run again without overwriting. This usually finishes the failed tests:</p> <pre><code>python tools/run_all_llm_as_a_judge_tests.py --config-file &lt;config&gt; --do-not-overwrite-eval-results\n</code></pre> <p>Example</p> <pre><code>python tools/run_all_llm_as_a_judge_tests.py -c tests/resources/llm_as_a_judge_datasets/classify_product/eval_set/config_open_ai.json\n</code></pre> <p>Warning</p> <p>Make sure to run with <code>-d</code> if you only want to rerun the failed tests, otherwise it will delete the previous results.</p> <p>Use the <code>--help</code> flag to see all the options, <code>-v</code> for verbose output.</p>"},{"location":"developers/testing-judges/#view-the-reports","title":"View the reports \ud83d\udcca","text":"<p>To open the reports for a specific judge use this script and select the judge you want to use:</p> <pre><code>python tools/show_reports_for_all_llm_as_a_judge_tests.py\n</code></pre> <p>Use the <code>--help</code> flag to see all the options.</p> <p></p>"},{"location":"getting-started/definitions/","title":"High-Level Concepts \ud83c\udf93","text":""},{"location":"getting-started/definitions/#evaluation-sets","title":"Evaluation Sets","text":"<p>An \"evaluation set\" or an \"eval set\" is your use case.</p> <p>It is the use case that you want to test your LLM against.</p> <p>For example:</p> <ul> <li>Detecting user actions in a screenshot using a vllm</li> <li>Answering questions about a product</li> </ul>"},{"location":"getting-started/definitions/#eval-set-dir-structure","title":"Eval-set Dir Structure","text":"<p>The eval set is a folder which name is the eval-set name. </p> <p>It includes:</p> <ul> <li><code>ground_truth.jsonl</code>: jsonl file with the list of tests and their expected results and optionally any additional data (payload) required for the task to run. The payload is the input for the task to run with.</li> <li><code>config.json</code> file: json config file for your eval-set, see the config section.</li> <li>a <code>testcases</code> folder that contains one or more \"testcase\" folders - more on that below.</li> </ul> <p>Here is an example of the directory structure for an eval set:</p>"},{"location":"getting-started/definitions/#testcases","title":"Testcases","text":"<p>A \"testcase\" is the set of prompt, parameters and code that affect the llm results.</p> <p>For example, a testcase could be using sonnet3.5 model with a specific prompt to evaluate your LLM, versus using gpt-o1 with its own prompt, or using sonnet3.5 with a different prompt.  It could also be different model parameters like temperature, top_k, top_p, or anything else that can affect the llm results.</p> <p>In this case you would probably name your testcases accordingly:</p> <ul> <li><code>sonnet35-prompt1</code></li> <li><code>sonnet35-prompt2</code></li> <li><code>gpt-o1-prompt1</code></li> </ul>"},{"location":"getting-started/definitions/#llm-tasks","title":"LLM Tasks","text":"<p>An \"llm task\" is the task that you run under this testcase. It is the logic that's called in your <code>task_handler.py</code> implementation. In case you already have the results, you can simply return them in <code>task_handler.py</code>.</p>"},{"location":"getting-started/definitions/#testcase-dir-structure","title":"Testcase Dir Structure","text":"<p>A testcase is a folder that includes:</p> <ul> <li><code>task_handler.py</code> - the implementation of the task runner. See the Implementing Handlers section.</li> <li><code>llm_task_results.jsonl</code> - Results for your llm tasks run - generated after you run at least once.</li> <li><code>llm_task_errors.txt</code>, <code>eval_errors.txt</code> - errors from your llm and eval runs in case there are any. Use for troubleshooting.</li> </ul>"},{"location":"getting-started/definitions/#eval-set-dir-structure-example","title":"Eval-set Dir Structure Example","text":"<p>Consider an eval-set named user-actions-by-events, with the testcases mentioned above. Here is the expected directory structure:</p> <pre><code>user-actions-by-events/\n\u251c\u2500\u2500 ground_truth.jsonl\n\u251c\u2500\u2500 config.json\n\u2514\u2500\u2500 testcases/\n  \u251c\u2500\u2500 sonnet35-prompt1/\n  \u2502   \u251c\u2500\u2500 task_handler.py\n  \u2502   \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500 sonnet35-prompt2/\n  \u2502   \u251c\u2500\u2500 task_handler.py\n  \u2502   \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500 ...\n  \u251c\u2500\u2500 gpt-o1-prompt1/\n  \u2502   \u251c\u2500\u2500 task_handler.py\n  \u2502   \u2514\u2500\u2500 ...\n  \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"getting-started/definitions/#llm-as-a-judge","title":"LLM as a Judge","text":"<p>This is a technique to evaluate LLMs by using LLM models as judges. They include metrics for which you want to evaluate your LLMs against.</p>"},{"location":"getting-started/definitions/#llm-as-a-judge-model","title":"LLM as a Judge Model","text":"<p>The built-in llm as a judge is implemented using Claude Sonnet 3.5. To see available models, run:</p> <pre><code>simpleval list-models\n</code></pre>"},{"location":"getting-started/definitions/#llm-as-a-judge-metrics","title":"LLM as a Judge Metrics","text":"<p>We support different LLM as a Judge metrics, such as <code>correctness</code>, <code>relevance</code> and more. Not all metrics are useful in all cases. To learn more about the available metrics, use the metrics explorer command:</p> <pre><code>simpleval metrics-explorer\n</code></pre> <p></p>"},{"location":"getting-started/init/","title":"Init a New Evaluation Set \ud83c\udfac","text":"<p>You have two ways to create a new evaluation set:</p>"},{"location":"getting-started/init/#option-1-interactive-mode-recommended","title":"Option 1: Interactive Mode (Recommended \u2b50\ufe0f)","text":"<p>This option will walk you through each configuration step interactively.</p> <p>Run the init command and follow the on-screen instructions:</p> <pre><code>simpleval init\n</code></pre> <p>Info</p> <ul> <li>When you select the judge, <code>simpleval</code> will test for working credentials, and if not available will show the required variables. You can continue the init process and set the variables later.</li> <li>You can configure how many judge tasks or llm tasks (your logic that you test for) to run concurrently. The default is 10 for both.</li> </ul>"},{"location":"getting-started/init/#option-2-init-from-template","title":"Option 2: Init From Template","text":"<p>This will create a new eval set directory from a template using default values.  You will need to update the configuration according to the on screen instructions.</p> <p>Run the init command:</p> <pre><code>simpleval init-from-template --eval-dir &lt;eval-dir-name&gt; --testcase &lt;testcase-name&gt;\n</code></pre> <p>For example:</p> <pre><code>simpleval init --eval-dir user-actions-by-events --testcase sonnet35-prompt1\n</code></pre> <p>And follow the on-screen instructions.</p> <p>The basic and mandatory steps are:</p>"},{"location":"getting-started/init/#updating-the-metrics-in-configjson","title":"Updating the metrics in <code>config.json</code>","text":"<p>Run <code>simpleval metrics-explorer</code> to see the available metrics and select the ones that fit your use-case. For more information see the config section.</p>"},{"location":"getting-started/init/#populate-ground_truthjsonl","title":"Populate <code>ground_truth.jsonl</code>","text":"<p>This is a JSONL file with the list of tests to run; each line is a valid JSON object. An example <code>ground_truth.jsonl</code> file:</p> <pre><code>{ \"name\": \"test1\", \"description\": \"description 1\", \"expected_result\": \"result1\", \"payload\": { \"param1\": \"value1\", \"param2\": \"value2\" } }\n{ \"name\": \"test2\", \"description\": \"description 2\", \"expected_result\": \"result2\", \"payload\": { \"param1\": \"value1\", \"param2\": \"value2\" } }\n</code></pre>"},{"location":"getting-started/init/#implement-the-llm-task-logic","title":"Implement the LLM Task Logic","text":"<p>\u2705 In your <code>&lt;your eval set&gt;/datasets/&lt;your dataset name&gt;/task_handler.py</code> file, implement your logic.</p> <p><code>task_logic(name: str, payload: dict)</code></p> <p>\u2705 Call your llm using your prompts, and the payload from the ground truth file.</p> <p>\u2705 You must return a <code>simpleval.testcases.schemas.llm_task_result.LlmTaskResult</code> object.</p> <pre><code>result = LlmTaskResult(\n    name=name,\n    prompt=your_prompt_for_the_llm,\n    prediction=llm_response,\n    payload=payload,\n)\n</code></pre> <ul> <li><code>name</code>, <code>payload</code> - send the name, payload task_logic in params as is</li> <li><code>prompt</code> - the prompt that you sent to the llm</li> <li><code>prediction</code> - the response from the llm call</li> </ul> <p>Retry Decorator</p> <p>It is recommended to implement a retry decorator for your function to handle transient errors such as rate limit errors.  If you are using Bedrock or LiteLLM, use @bedrock_limits_retry or @litellm_limits_retry  Otherwise implement your own.</p> <p>To debug your code, see the Debugging Handlers section</p> <p>Breaking Changes</p> <p>IMPORTANT: Breaking changes in simpleval might break your plugin. If you get a \"PLUGIN ERROR\", check the error message. Usually it is import issues, or changes to LlmTaskResult.</p>"},{"location":"getting-started/init/#on-screen-instructions","title":"On-Screen Instructions","text":"<p>After running the init command you'll have everything you need to know on screen:</p> <p></p> <p>You are now ready to run your first evaluation!</p>"},{"location":"getting-started/judge-authentication/","title":"Judge Models and Authentication  \ud83d\udc69\u200d\u2696\ufe0f\ud83d\udd11","text":"<p>Simpleval supports several judges, each one with its own authentication method. The way to authenticate with each judge is usually via environment variables or similar implicit methods. There might also be judge-specific details found below.</p> <p>You can see how each judge type is authenticated below, but in short, most judges are based on Lite LLM, so you can always also refer to its documentation.</p> <p>Tip</p> <p>You can view supported model ids for Lite LLM judges by running:</p> <pre><code>simpleval litellm-models-explorer\n</code></pre> Anthropic Judge <p>Required environment variable:</p> <pre><code>ANTHROPIC_API_KEY\n</code></pre> <p>Example:</p> <pre><code>ANTHROPIC_API_KEY=&lt;your-key&gt;\n</code></pre> <p>See the Anthropic docs for more details.</p> Azure Judge <p>Required environment variables:</p> <pre><code>AZURE_OPENAI_API_KEY\nAZURE_API_VERSION\nAZURE_API_BASE\n</code></pre> <p>Example:</p> <pre><code>AZURE_OPENAI_API_KEY=&lt;your-key&gt;\nAZURE_API_VERSION=2024-04-01-preview\nAZURE_API_BASE=https://&lt;your_resource_name&gt;.openai.azure.com/\n</code></pre> <p>Available Versions</p> <p>You can see available versions in the Azure docs</p> Bedrock Claude Sonnet Judge <p>This judge is NOT based on Lite LLM, it is implemented natively with boto3 calling Bedrock. </p> <p>Required credentials:</p> <p>AWS credentials must be available, either via environment variables or in a <code>~/.aws/credentials</code> file.</p> <p>You can learn more about using AWS credentials in the AWS docs</p> Gemini Judge <p>Required environment variable:</p> <pre><code>GEMINI_API_KEY\n</code></pre> <p>See the Gemini API docs for more details.</p> Generic Bedrock Judge <p>Required credentials:</p> <p>AWS credentials must be available, either via environment variables or in a <code>~/.aws/credentials</code> file.</p> <p>You can learn more about using AWS credentials in the AWS docs</p> LiteLLM Structured Output Judge <p>Required authentication:</p> <p>Depends on the provider you use with LiteLLM.</p> <p>You can learn about the available providers in the LiteLLM documentation.</p> <p>And about structured output in LiteLLM here</p> OpenAI Judge <p>Required environment variable:</p> <pre><code>OPENAI_API_KEY\n</code></pre> <p>See the OpenAI API docs for more details.</p> Vertex AI Judge <p>Required environment variable:</p> <pre><code>GOOGLE_APPLICATION_CREDENTIALS\nVERTEXAI_LOCATION\nVERTEXAI_PROJECT\n</code></pre> <p>Example:</p> <pre><code>GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json\nVERTEXAI_LOCATION=us-central1\nVERTEXAI_PROJECT=your-project-id\n</code></pre> <p>See the Vertex AI docs for more details.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart \ud83c\udfc3\u200d\u2642\ufe0f\u200d\u27a1\ufe0f","text":""},{"location":"getting-started/quickstart/#requirements","title":"Requirements","text":"<p>\u2705 Python 3.11 or later \ud83d\udc0d</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<p>\u2705 Install the package using <code>pip</code>, <code>uv</code> (or with your favorite package manager):</p> Python Native <p>Recommended to use a virtual environment:</p> <p>Create the virtual environment: <pre><code>python -m venv .venv\n</code></pre></p> <p>Activate the virtual environment:</p> Linux/Mac <pre><code>source .venv/bin/activate \n</code></pre> Windows <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>Install the package:</p> <pre><code>pip install simpleval\n</code></pre> uv package manager <p>Using a virtual environment:</p> <pre><code>uv .venv\n</code></pre> <p>Activate the virtual environment:</p> Linux/Mac <pre><code>source .venv/bin/activate \n</code></pre> Windows <pre><code>.venv\\Scripts\\activate\n</code></pre> <p>Install the package:</p> <pre><code>uv add simpleval\n</code></pre>"},{"location":"getting-started/quickstart/#usage","title":"Usage","text":"<p>\u2705 Run <code>simpleval init</code> and follow the instructions, or instead, follow the Tutorial section.</p> <p>To see all available commands, run <code>simpleval</code></p> <p> </p> <p>NOTE</p> <p>Reading has many benefits. To get a better understanding and a more detailed guide, it is recommended to go through each item under the <code>Getting Started</code> section in the sidebar in order.</p> <p></p>"},{"location":"getting-started/reporting/","title":"Reporting \ud83d\udcca","text":"<p>Currently, there are three types of reports:</p> <ul> <li>Eval results</li> <li>Comparison report</li> <li>Summary report</li> </ul> <p>List report types:</p> <pre><code>simpleval reports\n</code></pre>"},{"location":"getting-started/reporting/#report-format","title":"Report Format","text":"<p>Usually you can select the report format with the <code>--report-format console</code> (<code>-r console</code>) argument:</p> <ul> <li><code>html</code> - React based HTML report (default)</li> <li><code>console</code> - Console report</li> </ul>"},{"location":"getting-started/reporting/#output-directory","title":"Output Directory","text":"<p>Reports are saved to <code>results</code> directory and can be considered as temporary files (they can be regenerated easily again)</p>"},{"location":"getting-started/reporting/#eval-run-report","title":"Eval Run Report \ud83c\udfc3\u200d\u2642\ufe0f\u200d\u27a1\ufe0f","text":"<p>This is the basic report; it is generated when the run command finishes. If you just want to re-generate the report, you can use the <code>reports eval</code> command instead of running the entire evaluation.</p> <pre><code>simpleval reports eval -e &lt;eval_dir&gt; -t &lt;testcase&gt;\n</code></pre> <p>You can also run it by passing the eval name and results file directly:</p> <pre><code>simpleval reports eval-file -n &lt;eval-name&gt; -f &lt;eval_file&gt;\n</code></pre> <p></p>          Figure: Example of an Eval Run Report in HTML format"},{"location":"getting-started/reporting/#results-comparison-report","title":"Results Comparison Report \ud83d\udd2c","text":"<p>After you run more than one testcase evaluation, you can compare the results by running the compare command:</p> <p>Run:</p> <pre><code>simpleval reports compare -e &lt;eval_dir&gt; -t1 &lt;testcase1&gt; -t2 &lt;testcase2&gt;\n</code></pre> <p>t1 and t2 are two testcases under the same evaluation set directory (they must have the same ground truths).</p> <p>Better results are highlighted in green.</p> <p>You can also run it by passing the eval name and results file directly:</p> <pre><code>simpleval reports compare-files -n &lt;eval-name&gt; -f1 &lt;eval_file1&gt; -f2 &lt;eval_file2&gt;\n</code></pre> <p></p>          Figure: Example of a Result Comparison in HTML format"},{"location":"getting-started/reporting/#summary-report","title":"Summary Report \ud83d\udcca","text":"<p>You can also generate an overview summary of all testcases results by running:</p> <pre><code>simpleval reports summarize -e &lt;eval_dir&gt; -p &lt;metric name&gt;\n</code></pre> <p></p>          Figure: Example of a summary report in HTML format      <p></p>"},{"location":"getting-started/running/","title":"Running an evaluation \ud83c\udfc3\u200d\u2642\ufe0f\u200d\u27a1\ufe0f","text":"<p>After you go through the init process, updated your config file if needed, defined your ground truth file and implemented your task logic, you are ready to run your first evaluation.</p> <p>Run:</p> <p><pre><code>simpleval run -e &lt;eval_dir&gt; -t &lt;testcase&gt;\n</code></pre> A report with the results and calculated scores will open and saved to <code>results</code> directory. It includes mean score (normalized) for each test and metric.</p>          Figure: Example of eval results report in HTML format      <p>Results are saved to:</p> <ul> <li>LLM task results: <code>&lt;eval_dir&gt;/testcases/&lt;testcase&gt;/llm_task_results.jsonl</code></li> <li>Evaluation results: <code>&lt;eval_dir&gt;/testcases/&lt;testcase&gt;/eval_results.jsonl</code></li> </ul> <p>Errors are saved to:</p> <ul> <li>LLM task errors: <code>&lt;eval_dir&gt;/testcases/&lt;testcase&gt;/llm_task_errors.txt</code></li> <li>Evaluation errors: <code>&lt;eval_dir&gt;/testcases/&lt;testcase&gt;/eval_errors.txt</code></li> </ul>"},{"location":"getting-started/running/#handling-errors","title":"Handling errors \ud83d\udc1e","text":"<p>The report will include the number of errors. You might not care if some failed, but if you do:</p> <ul> <li>In case they are transient errors, like throttling or random parsing issues, simply run the eval again. It will run only on the missing tests.</li> </ul> <p>Warning</p> <p>Never run with the <code>--overwrite</code> or <code>-o</code> flag if you want to retry, otherwise, it will overwrite all existing results and run everything again.</p> <ul> <li>In case they are actual error, look in the error files (location above) and solve the issue.</li> </ul> <p>Tip</p> <p>You can use the <code>--verbose</code> or <code>-v</code> flag to get more detailed information about the errors. This will also make the error files more detailed.</p>"},{"location":"getting-started/running/#re-running-an-evaluation","title":"Re-running an evaluation","text":"<p>If you made changes to your prompts/model params/logic/etc, you want to re-run the entire evaluation. Do this by using the <code>--overwrite</code> or <code>-o</code> flag:</p> <pre><code>simpleval run -e &lt;eval_dir&gt; -t &lt;testcase&gt; -o\n</code></pre>"},{"location":"getting-started/running/#generating-the-report","title":"Generating the report","text":"<p>If you only want to generate a report from existing results, run:</p> <p><pre><code>simpleval reports eval -e &lt;eval_dir&gt; -t &lt;testcase&gt;\n</code></pre> </p>"},{"location":"getting-started/tutorial/","title":"Tutorial \ud83d\udc69\u200d\ud83c\udfeb","text":"<p>After you go through the Quickstart section, you can follow this tutorial for a quick example.</p> <p>Make sure you can run <code>simpleval</code> before you continue.</p> <p>This can be in a python virtual environment (recommended) or as a cli tool.</p>"},{"location":"getting-started/tutorial/#the-scenario","title":"The Scenario \ud83e\uddfe","text":"<p>You are testing whether an LLM can answer questions about a short story called \"The Clockmaker's Secret\"<sup>1</sup></p> <p>You will compare how two different prompts are doing by using <code>simpleval</code> to evaluate and compare their results. This is done with the \"LLM as a judge\" technique.</p>"},{"location":"getting-started/tutorial/#setup","title":"Setup \u2699\ufe0f","text":""},{"location":"getting-started/tutorial/#set-llm-credentials","title":"\u2705 Set LLM credentials","text":"<p>Make sure you have the credentials set for the LLM you want to use. See supported providers and their required credentials in the Judge Models and Authentication section.</p>"},{"location":"getting-started/tutorial/#init-the-evaluation-set","title":"\u2705 Init the evaluation set","text":"<ul> <li> Run the interactive init command:</li> </ul> <pre><code>simpleval init\n</code></pre> <ul> <li> For <code>Enter eval folder...:</code> enter <code>story-q-and-a</code></li> <li> For <code>Enter test case name (enter to stop):</code> enter <code>prompt1</code></li> <li> Select a judge provider you want to use, which ever you have access to. For example <code>open-ai</code>       Don't worry if you get an error saying that the necessary credentials are not set; you can set them later.       Just enter 'y' to continue.</li> </ul> <p>notice</p> <p>Make sure you have the credentials set for the judge model you selected before you run the evaluation, if not you will be shown the expected environment variables to set for the judge model.</p> <ul> <li> Select the recommended model id to use</li> <li> Select <code>Pick your own metrics</code> and select: <code>\"correctness\", \"relevance\", \"completeness\", \"readability\"</code> (if you make a mistake, you can always edit the <code>config.json</code> file before you run)       Press <code>Enter</code> to continue.</li> <li> Skip <code>Do you want to configure concurrency</code> by hitting <code>enter</code></li> </ul> <p>This will result in a folder structure like this:</p> <pre><code>story-q-and-a\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 ground_truth.jsonl\n\u251c\u2500\u2500 ...\n\u2514\u2500\u2500 testcases\n    \u2514\u2500\u2500 prompt1\n        \u2514\u2500\u2500 task_handler.py\n</code></pre> <ul> <li> \u26a0\ufe0f If you haven't done so already, now is the time to set the credentials for the judge model you selected.</li> </ul>"},{"location":"getting-started/tutorial/#create-storytxt","title":"\u2705 Create <code>story.txt</code>","text":"<p>We want to provide the story to our LLM as a judge</p> <ul> <li> Create a file named <code>story.txt</code> in the <code>story-q-and-a/testcases</code> folder</li> <li> Paste the following story into the file and save it.</li> </ul> <p>The story: <pre><code>The Clockmaker's Secret\n\nIn the quaint village of Greystone, an eccentric clockmaker named Elias Thorne was rumored to have crafted a clock that could alter time itself.\nOne day, a young woman named Clara entered his shop, searching for a gift for her father.\nAmong the countless clocks, she was drawn to a small, unassuming one with a single golden hand. When she asked about it, \nElias hesitated and warned her, \u201cIt doesn\u2019t just tell time\u2014it listens to it. Be careful with what you wish for.\u201d\nIntrigued, Clara bought the clock, dismissing his words as a playful superstition.\nThat night, Clara placed the clock on her bedside table and whispered, \u201cI wish I had more time with my father.\u201d\nThe clock stopped ticking, its golden hand spinning backward before the world around her blurred.\nShe found herself in her childhood home, her father\u2019s laughter filling the air.\nAt first, she was overjoyed to relive these moments, but soon she realized she was trapped in the past, the clock inexplicably following her wherever she went.\nPanicked, Clara returned to Elias, demanding to know how to undo her wish.\nThe clockmaker\u2019s face softened with sympathy as he said, \u201cTime only grants what is asked, not what is truly needed.\nTo move forward, you must let go of what holds you back.\u201d\nWith that cryptic advice, Clara clutched the clock tightly,\nunsure if she could bear to leave the past behind\u2014but knowing it was the only way to reclaim her future.\n</code></pre></p>"},{"location":"getting-started/tutorial/#prepare-the-ground-truth","title":"\u2705 Prepare the Ground Truth","text":"<ul> <li> Set the content of the <code>story-q-and-a/ground_truth.jsonl</code> file to:</li> </ul> <pre><code>{ \"name\": \"test1\", \"description\": \"Why did Clara feel trapped after using the clock\", \"expected_result\": \"Clara initially enjoyed reliving moments with her father but soon realized she was stuck in the past and unable to return to the present. The clock\u2019s mysterious power had granted her wish literally, but it came at the cost of her ability to move forward in life.\", \"payload\": { \"question\": \"Why did Clara feel trapped after using the clock?\" } }\n{ \"name\": \"test2\", \"description\": \"What advice did Elias give Clara to fix her situation?\", \"expected_result\": \"Elias told Clara, \u201cTime only grants what is asked, not what is truly needed. To move forward, you must let go of what holds you back.\u201d This implied that Clara had to release her emotional attachment to the past to break free from its hold and return to the present.\", \"payload\": { \"question\": \"What advice did Elias give Clara to fix her situation?\" } }\n{ \"name\": \"test3\", \"description\": \"What might the clock symbolize in the story?\", \"expected_result\": \"The clock symbolizes the passage of time and the danger of dwelling too much on the past. It serves as a reminder that while we may wish to revisit cherished memories, clinging to them can prevent us from living in the present and embracing the future.\", \"payload\": { \"question\": \"What might the clock symbolize in the story?\" } }\n</code></pre> <p>These are three questions and their expected answers that the judge LLM would check against.</p>"},{"location":"getting-started/tutorial/#implement-prompt1-handler","title":"\u2705 Implement <code>prompt1</code> handler","text":"<p>Info</p> <p>Below you can find code samples for implementing your plugin with the popular LLM providers. Most examples use LiteLLM (which is installed with <code>simpleval</code>), but there are also examples for Bedrock, and OpenAI SDK.</p> <ul> <li> Copy the code below to <code>story-q-and-a/testcases/prompt1/task_handler.py</code> according to the LLM provider you are using.</li> </ul> <p>This code is calling an LLM, asking it to answer the provided question about the story. The simple prompt is set at <code>prompt</code>, instructing the LLM to answer the question about the story.</p> OpenAIAnthropicGeminiBedrock (Sonnet)AzureVertex AIOpenAI (SDK) <pre><code>import os\nimport logging\n\nfrom litellm import completion, ModelResponse\n\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\nfrom simpleval.utilities.retryables import litellm_limits_retry\n\nmodel_id = 'gpt-4.1-mini'\ntemperature = 0.7\n\n\ndef call_completion(prompt: str) -&gt; str:\n    print('Call to completion started')\n\n    response: ModelResponse = completion(\n        model=model_id,\n        temperature=temperature,\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n    )\n\n    output = response.choices[0].message.content\n\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    return output\n\n\n@litellm_limits_retry\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre> <pre><code>import os\nimport logging\n\nfrom litellm import completion, ModelResponse\n\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\nfrom simpleval.utilities.retryables import litellm_limits_retry\n\nmodel_id = 'claude-3-5-haiku-latest'\ntemperature = 0.7\n\n\ndef call_completion(prompt: str) -&gt; str:\n    print('Call to completion started')\n\n    response: ModelResponse = completion(\n        model=model_id,\n        temperature=temperature,\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n    )\n\n    output = response.choices[0].message.content\n\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    return output\n\n\n@litellm_limits_retry\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre> <pre><code>import os\nimport logging\n\nfrom litellm import completion, ModelResponse\n\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\nfrom simpleval.utilities.retryables import litellm_limits_retry\n\nmodel_id = 'gemini/gemini-2.0-flash'\ntemperature = 0.7\n\n\ndef call_completion(prompt: str) -&gt; str:\n    print('Call to completion started')\n\n    response: ModelResponse = completion(\n        model=model_id,\n        temperature=temperature,\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n    )\n\n    output = response.choices[0].message.content\n\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    return output\n\n\n@litellm_limits_retry\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre> <pre><code>import json\nimport logging\n\nimport boto3\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\nfrom simpleval.utilities.retryables import bedrock_limits_retry\nimport os\n\n\nclient = boto3.client(\"bedrock-runtime\")\n\nmodel_id = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\naccept = 'application/json'\ncontent_type = 'application/json'\n\ndef get_claude_body_dict(sys_prompt: str, user_prompt: str) -&gt; dict:\n    body_dict = {\n        'anthropic_version': 'bedrock-2023-05-31',\n        'system': sys_prompt.strip(),\n        'max_tokens': 8192,\n        'messages': [{\n            'role': 'user',\n            'content': [{\n                'type': 'text',\n                'text': user_prompt.strip()\n            }],\n        },\n        {\n            'role': 'assistant',\n            'content': [{\n                'type': 'text',\n                'text': '[Eager reader]'\n            }],\n    }\n        ],\n    }\n\n    body_dict['temperature'] = 0.7\n    return body_dict\n\n\ndef call_claude_completion(system_prompt):\n    print('Call to Claude completion started')\n\n    user_prompt = \"answer the question\"\n\n    body_dict = get_claude_body_dict(system_prompt, user_prompt)\n    body = json.dumps(body_dict)\n\n    response = client.invoke_model(body=body, modelId=model_id, accept=accept, contentType=content_type)\n\n    result = json.loads(response.get('body').read())\n    input_tokens = result.get('usage', {}).get('input_tokens', '')\n    output_tokens = result.get('usage', {}).get('output_tokens', '')\n    output_list = result.get('content', [])\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    if not output_list:\n        print('empty response')\n    else:\n        output = output_list[0].get('text', '')\n\n        # Note that if you include the { as the prefill, it will not be included in the response so we need it ourselves, see cookbook link:\n        # https://github.com/anthropics/anthropic-cookbook/blob/main/misc/how_to_enable_json_mode.ipynb\n        # output = '{' + output\n    return output\n\n@bedrock_limits_retry\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_claude_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre> <pre><code>import os\nimport logging\n\nfrom litellm import completion, ModelResponse\n\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\nfrom simpleval.utilities.retryables import litellm_limits_retry\n\nmodel_id = 'azure/gpt-4.1-mini'\ntemperature = 0.7\n\n\ndef call_completion(prompt: str) -&gt; str:\n    print('Call to completion started')\n\n    response: ModelResponse = completion(\n        model=model_id,\n        temperature=temperature,\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n    )\n\n    output = response.choices[0].message.content\n\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    return output\n\n\n@litellm_limits_retry\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre> <pre><code>import os\nimport logging\n\nfrom litellm import completion, ModelResponse\n\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\nfrom simpleval.utilities.retryables import litellm_limits_retry\n\nmodel_id = 'vertex_ai/gemini-2.0-flash'\ntemperature = 0.7\n\n\ndef call_completion(prompt: str) -&gt; str:\n    print('Call to completion started')\n\n    response: ModelResponse = completion(\n        model=model_id,\n        temperature=temperature,\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n    )\n\n    output = response.choices[0].message.content\n\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    return output\n\n\n@litellm_limits_retry\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre> <pre><code>import logging\nimport os\n\nfrom openai import OpenAI, ChatCompletion\n\nfrom simpleval.consts import LOGGER_NAME\nfrom simpleval.testcases.schemas.llm_task_result import LlmTaskResult\n\nclient = OpenAI()\n\nmodel_id = 'gpt-4.1-mini'\ntemperature = 0.7\n\n\ndef call_completion(prompt: str) -&gt; str:\n    print('Call to completion started')\n\n    completion: ChatCompletion = client.chat.completions.create(\n        model=model_id,\n        temperature=temperature,\n        messages=[{\n            'role': 'user',\n            'content': prompt\n        }],\n    )\n\n    output = completion.choices[0].message.content\n    input_tokens = completion.usage.prompt_tokens\n    output_tokens = completion.usage.completion_tokens\n\n    print(f'{input_tokens=}, {output_tokens=}')\n\n    return output\n\n\n# Implement your own retry mechanism\ndef task_logic(name: str, payload: dict) -&gt; LlmTaskResult:\n    logger = logging.getLogger(LOGGER_NAME)\n    logger.debug(f'{__name__}: Running task logic for {name} with payload: {payload}')\n\n    story_file_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'story.txt')\n    with open(story_file_path, 'r') as file:\n        story_content = file.read()\n\n    prompt = f'Read this short story and answer the question at the end. Story: {story_content}. Question: {payload[\"question\"]}'\n\n    llm_response = call_completion(prompt)\n    print(llm_response)\n\n    result = LlmTaskResult(\n        name=name,\n        prompt=prompt,  # This is what you sent to your llm\n        prediction=llm_response,  # This is what your llm responded\n        payload=payload,\n    )\n\n    return result\n</code></pre>"},{"location":"getting-started/tutorial/#implement-prompt2-handler","title":"\u2705 Implement <code>prompt2</code> handler","text":"<p>Now we will implement <code>prompt2</code>: a pirate who answers the question about the story.</p> <ul> <li> <p> Copy the <code>prompt1</code> directory to <code>prompt2</code>.</p> </li> <li> <p> Update <code>prompt</code> in <code>story-q-and-a/testcases/prompt2/task_handler.py</code> to:</p> </li> </ul> <pre><code>prompt = f'Read this short story and answer the question at the end You must use pirate language in your responses. Story: {story_content}. Question: {payload[\"question\"]}'\n</code></pre>"},{"location":"getting-started/tutorial/#final-directory-structure","title":"Final Directory Structure","text":"<p>Your directory structure should look like this:</p> <pre><code>story-q-and-a\n\u251c\u2500\u2500 config.json\n\u251c\u2500\u2500 ground_truth.jsonl\n\u251c\u2500\u2500 ...\n\u2514\u2500\u2500 testcases\n    \u251c\u2500\u2500 story.txt\n    \u251c\u2500\u2500 prompt1\n    \u2502   \u2514\u2500\u2500 task_handler.py\n    \u2514\u2500\u2500 prompt2\n         \u2514\u2500\u2500 task_handler.py\n</code></pre>"},{"location":"getting-started/tutorial/#running-the-evaluation","title":"Running the Evaluation \ud83c\udfc3\u200d\u2642\ufe0f","text":""},{"location":"getting-started/tutorial/#running-the-eval-process","title":"\u2705 Running the eval process","text":"<ul> <li> Verify that you have the credentials set for running the LLM and LLM-as-a-judge (API key, for example)</li> <li> Run for the two testcases:</li> </ul> <pre><code>simpleval run -e story-q-and-a -t prompt1\n</code></pre> <pre><code>simpleval run -e story-q-and-a -t prompt2\n</code></pre> <p>In case of errors, look in the error logs for the test case you ran, for example:</p> <p><code>story-q-and-a/testcases/prompt1/llm_task_errors.txt</code> </p> <p>and </p> <p><code>story-q-and-a/testcases/prompt1/eval_errors.txt</code>.</p> <p>Fix the issues and run again. simpleval will only run the failing tests.</p> <p>Tip</p> <p>If for any reason you need to run everything, overwriting all previous results, use the -o flag: <pre><code>simpleval run -e story-q-and-a -t prompt1 -o\n</code></pre> \u26a0\ufe0f Just keep in mind that this will overwrite all previous results.</p>"},{"location":"getting-started/tutorial/#review-the-results","title":"\u2705 Review the results","text":"<ul> <li> Go over the result reports for each prompt.</li> <li> You can also run a head-to-head comparison by running:</li> </ul> <pre><code>simpleval reports compare -e story-q-and-a -t1 prompt1 -t2 prompt2\n</code></pre> <ul> <li> You can also create a summary report to see all testcases (you can optionally specify a primary metric to focus on, such as <code>readability</code>):</li> </ul> <pre><code>simpleval reports summarize -e story-q-and-a [--primary-metric readability]\n</code></pre> <p> Your LLM as a judge should detect the pirate language in the second prompt and score it accordingly, usually under the <code>readability</code> metric, as you can see here:</p>          Figure: Comparison Report highlighting the readability metric      <p></p> <p>Congratulations!</p> <p>\ud83c\udf89 You have completed the tutorial. Go evaluate some real stuff! \ud83c\udf89</p>"},{"location":"getting-started/tutorial/#troubleshooting","title":"Troubleshooting \ud83d\udd75\ufe0f\u200d\u2642\ufe0f","text":"<ul> <li>Evaluation finished with errors: Check the <code>llm_task_errors.txt</code> and <code>eval_errors.txt</code> files in the test case folders (<code>story-q-and-a/testcases/prompt1/</code> and <code>story-q-and-a/testcases/prompt2/</code>).</li> <li>Transient errors: In case of transient errors like rate limits or rare parsing errors, you can simply run the evaluation again; only the failed test cases will run.</li> <li>Verbose logging: You can always run with <code>-v</code> to get more verbose output.</li> </ul> <ol> <li> <p>\"The Clockmaker's Secret\" is a silly story generated by ChatGPT.\u00a0\u21a9</p> </li> </ol>"},{"location":"maintainers/version-release/","title":"Version Release \ud83d\ude80","text":"<p>Versioning Schema</p> <p>simpleval versioning follows Semantic Versioning.</p> <p>Stable versions</p> <p><code>MAJOR.MINOR.PATCH</code> where:</p> <ul> <li><code>MAJOR</code> version when you make incompatible or breaking changes</li> <li><code>MINOR</code> version when you add functionality in a backwards-compatible manner</li> <li><code>PATCH</code> version when you make backwards-compatible bug fixes and minor changes</li> </ul> <p>Examples: <code>1.0.0</code>, <code>1.0.1</code>, <code>2.0.0</code></p> <p>Pre-release versions</p> <p>For pre-release versions, append <code>-alpha.x</code>, <code>-beta.x</code>, or <code>-rc.x</code> (only those three are allowed), where alpha is for early testing, beta is for more stable testing, and rc is for release candidate.</p> <p>Examples: <code>1.0.0-alpha.1</code>, <code>1.0.0-beta.1</code>, <code>1.0.0-rc.1</code></p>"},{"location":"maintainers/version-release/#release-procedure","title":"Release Procedure","text":""},{"location":"maintainers/version-release/#release-automation","title":"Release Automation","text":"<p>Automatic Release Procedure (Recommended)</p> <ul> <li> Run the Init Release Workflow workflow and wait for it to complete.</li> </ul> Init Release Workflow - Under the Hood <p>The Init Release Workflow will:</p> <ul> <li>Creates a release branch named <code>release/auto-next-version</code> (or provide your own branch name)</li> <li>Bump patch/minor/major or specific version (set <code>version_bump</code> and optionally <code>version</code>)</li> <li>Update <code>CHANGELOG.md</code> for this release</li> <li>Create a pull request with the changes</li> </ul> <ul> <li> Review the pull request created by the workflow</li> <li> Optionally tweak <code>CHANGELOG.md</code> to your liking</li> <li> Merge the pull request to main - This will trigger the tag creation workflow</li> <li> Wait for the version tag creation workflow to complete</li> <li> Trigger the Release Workflow </li> <li> Manually publish the release to PyPI</li> </ul> Release Workflow - Under the Hood <p>Since operations done with internal tokens will not by default trigger workflows, the Release workflow is triggered manually.</p> <p>The release workflow will:</p> <ul> <li>Validate the <code>pyproject.toml</code> version against the tag name</li> <li>Build the package</li> <li>Create GitHub release</li> <li>Attach binaries to GitHub release</li> <li>Publish the docs in case of release version (pre-release versions will not publish the docs)</li> </ul>"},{"location":"maintainers/version-release/#docs-only-release","title":"\ud83d\udcda Docs Only Release","text":"<p>The release workflow will deploy the docs automatically for stable releases, so if you want to publish the docs for a pre-release version, you can do so by running the Publish Docs Workflow</p> <p>To learn more about the docs publishing, see Publishing the Docs).</p> <p>NOTE: Docs publishing is done only for the top level version (release or pre-release)</p>"},{"location":"maintainers/version-release/#obsolete-release-procedure-go-no-further","title":"\ud83d\udc80 Obsolete Release Procedure - Go No Further \ud83d\udc80","text":""},{"location":"maintainers/version-release/#run-the-release-procedure-with-scripts-not-recommended","title":"Run the Release Procedure with Scripts (Not Recommended)","text":"<p>Using the Release Scripts</p> <ul> <li> Run the release procedure script in one of the following ways:</li> </ul> <pre><code>./ci/scripts/create_version_pr.py --version 1.0.0-rc5g # Set a specific version (good for pre-release)\n./ci/scripts/create_version_pr.py --bump-patch\n./ci/scripts/create_version_pr.py --bump-minor\n./ci/scripts/create_version_pr.py --bump-major\n</code></pre> <ul> <li> Manually open the release notes, click on \"generate release notes\" and update them as needed</li> <li> After the release workflow is complete, update the <code>CHANGELOG.md</code> file with the new version and the changes made in this release (take from the release notes)</li> <li> Optionally review and update the release note in the release</li> <li> Publish the package to PyPI</li> </ul>"},{"location":"maintainers/version-release/#the-manual-way-even-less-recommended","title":"\u274c The Manual Way (Even less Recommended)","text":""},{"location":"maintainers/version-release/#1-update-version-in-pyprojecttoml","title":"1. Update Version in <code>pyproject.toml</code>","text":"<p>Recommended: Run the create version PR script</p> <pre><code>./ci/scripts/create_version_pr.py\n</code></pre> <p>Otherwise follow these steps:</p> <ul> <li>Update <code>pyproject.toml</code>: Using <code>uv</code> Update the version according to the versioning schema above. Make sure you adhere to the versioning rules and only include alpha, beta, or rc for pre-release versions.</li> </ul> <p>examples: <pre><code>uv version --bump patch # 1.0.1 -&gt; 1.0.2\nuv version --bump minor # 1.0.1 -&gt; 1.1.0\nuv version --bump major # 1.0.1 -&gt; 2.0.0\n\nuv version 1.0.1-alpha.1 # set to pre-release version\nuv version 1.0.1-beta.1 # set to pre-release version\nuv version 1.0.1-rc.1 # set to pre-release version\n</code></pre></p> <ul> <li> <p>Run <code>uv sync</code> to update <code>uv.lock</code>.</p> </li> <li> <p>Create a pull request for the changes and merge to main.</p> </li> </ul>"},{"location":"maintainers/version-release/#2-create-a-release-tag","title":"2. Create a Release Tag","text":"<p>On the main branch, create a tag using the <code>ci/scripts/create_tag.py</code> script. The script will read the version from <code>pyproject.toml</code> and create a tag with the correct format.</p>"},{"location":"maintainers/version-release/#3-update-the-release-notes","title":"3. Update the Release Notes","text":"<ul> <li>Wait for the release workflow to end successfully</li> <li>Manually open the release notes, click on \"generate release notes\" and update them as needed</li> <li>In the future this will be automated</li> </ul>"},{"location":"maintainers/version-release/#4-update-the-changelog","title":"4. Update the Changelog","text":"<ul> <li>After the release workflow is complete, update the <code>CHANGELOG.md</code> file with the new version and the changes made in this release (take from the release notes).</li> </ul>"},{"location":"maintainers/version-release/#5-run-the-publish-pipeline","title":"5. Run the Publish Pipeline","text":"<p>Do it</p> <p></p>"},{"location":"users/bookkeeping/","title":"Bookkeeping \ud83d\udd22","text":"<p>By default the number of input/output tokens and the model used is logged using <code>logs/tokens-bookkeeping.log</code> It is called in each judge implementation and if other judges are created, you should also log their token usage.</p> <p>If you're using simpleval and implementing your own handlers, then it is advised to also log the token usage, as shown in <code>simpleval/eval_sets/empty/testcases/empty/task_handler.py</code></p> <p>The log format include the time, source (eval, handler etc), input and output tokens</p> time source model input_tokens output_tokens <code>2025-03-19 16:11:04</code> <code>eval</code> <code>gpt-4.1-2025-04-14</code> 267 127 <p><code>source</code> is determined by the caller of the <code>log_bookkeeping_data</code> function.</p> <p>Tip</p> <p>When implementing a new testcase handler (plugin) or a new judge, you should also log the token usage.  This is done by calling <code>log_bookkeeping_data</code> after your call to the model.</p>"},{"location":"users/bookkeeping/#bookkeeping-summary","title":"Bookkeeping summary","text":"<p>You can use the <code>tools/bookkeeping.py</code> to summarize the bookkeeping data.  This includes hard-coded pricing. Update as needed.</p> <p></p>"},{"location":"users/cli-commands/","title":"CLI Commands \ud83d\udda5\ufe0f","text":"<p>Learn all the options with the help command:</p> <pre><code>simpleval -h\n</code></pre> <p>List available commands:</p> <p><pre><code>simpleval\n</code></pre> </p>"},{"location":"users/cli-commands/#important-flags","title":"Important flags","text":""},{"location":"users/cli-commands/#verbosity","title":"Verbosity","text":"<p>All commands support the <code>--verbose</code> or <code>-v</code> flag to get more detailed output. This will also make the error files more detailed.</p>"},{"location":"users/cli-commands/#overwrite-results","title":"Overwrite results","text":"<p>Use <code>--overwrite</code> or <code>-o</code> with care to overwrite the results of the evaluation. This will delete all existing results and re-run the evaluation.</p> <p>Warning</p> <p>Do not use this if you only want to attempt failed tests.</p>"},{"location":"users/cli-commands/#report-console-output","title":"Report Console Output","text":"<p>All report commands (<code>run</code>, <code>reports eval</code>, <code>compare</code>, and their file counterparts) create an HTML report by default (React-based <code>html2</code> by default).</p> <p>If you want console output, you can also pass the <code>--report-format console</code> or <code>-r console</code> flag.</p>"},{"location":"users/configuration/","title":"Configuration \u2699\ufe0f","text":""},{"location":"users/configuration/#eval-set-configuration","title":"Eval Set Configuration \u26ed","text":"<p>The eval set configuration file allows you to set:</p> <ul> <li><code>name</code> - display name</li> <li><code>max_concurrent_judge_tasks</code>: how many judge tasks to run concurrently</li> <li><code>max_concurrent_llm_tasks</code>: how many llm tasks to run concurrently</li> <li><code>eval_metrics</code>: list of metrics to evaluate. to learn about the available metrics, run <code>simpleval metrics-explorer</code></li> <li><code>llm_as_a_judge_name</code>: the model to use for the llm as a judge. Run <code>simpleval list-models</code> to see available judges.</li> <li><code>llm_as_a_judge_model_id</code> (optional) - The model id to use for the <code>llm_as_a_judge_name</code> provider. Each judge comes with a default model-id, but you can set your own. Run <code>simpleval litellm-models-explorer</code> to see available models for Lite LLM judges.</li> </ul>"},{"location":"users/configuration/#override-eval-set-configuration","title":"Override Eval Set Configuration \u26ed","text":"<p>If you want to override the configuration for the testcase level, you can do this for <code>max_concurrent_judge_tasks</code> and/or <code>max_concurrent_llm_tasks</code> by setting the <code>override</code> config element with the testcase name you want to override.</p> <p>For example:</p> <pre><code>{\n  \"name\": \"detect_user_action\",\n  \"max_concurrent_judge_tasks\": 10,\n  \"max_concurrent_llm_tasks\": 10,\n  \"eval_metrics\": [\"correctness\", \"relevance\", \"completeness\"],\n  \"llm_as_a_judge_name\": \"open_ai\",\n  \"override\": {\n    \"nova_lite1\": {\n      \"max_concurrent_judge_tasks\": 5,\n      \"max_concurrent_llm_tasks\": 5\n    }\n  }\n}\n</code></pre>"},{"location":"users/configuration/#global-configuration","title":"Global Configuration \u26ed","text":"<p>The global configuration file allows you to override certain global settings.</p> <p>Currently it supports setting the retry behavior for different models.</p> <p>Create a <code>global_config.json</code> in your working directory and populate it as you wish. You should see an indication in the terminal that the global configuration file was loaded.</p>"},{"location":"users/configuration/#retries-configuration","title":"Retries Configuration \u26ed","text":"<p>Warning</p> <p>This is an experimental feature and is not yet fully supported.</p> <p>You can override the behavior of the retry mechanism for family of models.</p> <p>This is useful if you keep hitting rate limits.</p> <p>Supported models:</p> <p>\u2705 <code>bedrock_claude_sonnet</code> (will work for any bedrock model)</p> <p>Retries are using tenacity's <code>wait_random_exponential</code> function. To learn more about the available options, see the function documentation</p> <p>The default values are:</p> <pre><code>{\n  \"retry_configs\": {\n    \"bedrock\": {\n      \"stop_after_attempt\": 5,\n      \"multiplier\": 2,\n      \"min\": 10,\n      \"max\": 30,\n      \"exp_base\": 2\n    }\n  }\n}\n</code></pre> <p>This means that it will stop after 5 attempts, has an initial window of 2s, will increase the wait time exponentially by a factor of 2, with a minimum of 10 seconds and up to a maximum of 30 seconds.</p> <p>Update the values and paste into your global config file.</p> <p>The global config schema is implemented in: <code>simpleval/global_config/retries.py</code></p> <p></p>"},{"location":"users/debugging-handlers/","title":"Debugging Handlers \ud83d\udc1e","text":"<p>If you're using this library as a consumer and you want to debug your handler, you need to run the simpleval <code>main.py</code> file with the command line args.</p> <p>This file resides in: </p> <p><code>\"${workspaceFolder}/.venv/lib/python3.11/site-packages/simpleval/main.py\"</code></p> <p>Info</p> <ul> <li>update the path to .venv according to your actual one.</li> <li><code>${workspaceFolder}</code> - is specific to vscode, so change it according to your IDE</li> <li>replace <code>python3.11</code> with your version</li> <li>add args as needed.</li> </ul>"},{"location":"users/debugging-handlers/#vscode-debug-settings","title":"vscode debug settings","text":"<p>Update args as you wish</p> <pre><code>{\n  \"version\": \"0.2.0\",\n  \"configurations\": [\n    {\n      \"name\": \"Debug simpleval\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/.venv/lib/python3.11/site-packages/simpleval/main.py\",\n      \"console\": \"integratedTerminal\",\n      \"args\": [\n        \"run\",\n        \"-e\",\n        \"eval_set/user-actions-by-events\",\n        \"-t\",\n        \"sonnet35\",\n        \"-o\",\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"users/debugging-handlers/#other-ides","title":"Other IDEs","text":"<p>Other IDEs are a little different, but the idea is the same, debug <code>main.py</code> in said path with the command line args.</p> <p></p>"}]}